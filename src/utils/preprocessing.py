# src/utils/preprocessing.py

import os
import pandas as pd
from tqdm import tqdm
from src.feature_engineering import merge_all_features

def align_categorical_features(X_train, X_test, categorical_cols):
    """
    å°†æµ‹è¯•é›†ä¸­çš„åˆ†ç±»ç‰¹å¾ä¸è®­ç»ƒé›†å¯¹é½ï¼Œç¡®ä¿ç±»åˆ«ä¸€è‡´ã€‚
    é€‚ç”¨äº LightGBM ç­‰å¯¹ category ç±»å‹æ•æ„Ÿçš„æ¨¡å‹ã€‚
    """
    for col in categorical_cols:
        if col in X_test.columns and col in X_train.columns:
            if pd.api.types.is_categorical_dtype(X_train[col]):
                X_test[col] = pd.Categorical(X_test[col], categories=X_train[col].cat.categories)
    return X_test


# def generate_test_features(sample_submission_path, members_df, transactions_df, logs_df):
#     """
#     ä¸ºæ¯ä¸ªæµ‹è¯•ç”¨æˆ·åŠ¨æ€ç”ŸæˆçœŸå®çš„ last_expire_dateï¼Œç”¨äºè¡Œä¸ºæ•°æ®è¿‡æ»¤ã€‚
#     """
#     test_users = pd.read_csv(sample_submission_path)
#     test_df = test_users.copy()
#     test_df["is_churn"] = -1  # dummy label

#     # æå–æ¯ä¸ªæµ‹è¯•ç”¨æˆ·çš„æœ€æ–° membership_expire_date
#     transactions_df["membership_expire_date"] = pd.to_datetime(transactions_df["membership_expire_date"], format="%Y%m%d")
#     last_expire = transactions_df.groupby("msno")["membership_expire_date"].max().reset_index()
#     last_expire.columns = ["msno", "last_expire_date"]

#     # åˆå¹¶åˆ°æµ‹è¯•é›†
#     test_df = test_df.merge(last_expire, on="msno", how="left")

#     # å¦‚æœæŸäº›ç”¨æˆ·æ²¡æœ‰äº¤æ˜“è®°å½•ï¼Œé»˜è®¤è®¾ä¸º 2017-03-31ï¼ˆä¿å®ˆå¤„ç†ï¼‰
#     test_df["last_expire_date"] = test_df["last_expire_date"].fillna(pd.to_datetime("2017-03-31"))

#     expire_df = test_df[["msno", "last_expire_date"]].copy()
#     features = merge_all_features(test_df, members_df, transactions_df, logs_df, expire_df)

#     return features


def generate_test_features_cached(members_df, transactions_df, logs_df, sample_submission_path, cache_path, force_reload=False):
    test_users = pd.read_csv(sample_submission_path)
    test_df = test_users.copy()
    test_df["is_churn"] = -1  # dummy label

    transactions_df["membership_expire_date"] = pd.to_datetime(transactions_df["membership_expire_date"], format="%Y%m%d", errors="coerce")
    last_expire = transactions_df.groupby("msno")["membership_expire_date"].max().reset_index()
    last_expire.columns = ["msno", "last_expire_date"]

    test_df = test_df.merge(last_expire, on="msno", how="left")
    test_df["last_expire_date"] = test_df["last_expire_date"].fillna(pd.to_datetime("2017-03-31"))
    expire_df = test_df[["msno", "last_expire_date"]].copy()

    return get_or_cache_features(
        cache_path=cache_path,
        batch_mode=True,
        batch_args={
            "full_df": test_df,
            "members_df": members_df,
            "transactions_df": transactions_df,
            "logs_df": logs_df,
            "expire_df": expire_df,
            "batch_size": 50000
        },
        force_reload=force_reload
    )


def merge_all_features_in_batches(
    full_df,
    members_df,
    transactions_df,
    logs_df,
    expire_df,
    batch_size=50000
):
    """
    åˆ†æ‰¹è¿è¡Œ merge_all_featuresï¼Œé™ä½å†…å­˜å ç”¨ã€‚
    å‚æ•°ï¼š
        - full_df: åŒ…å« msnoã€is_churn çš„ DataFrameï¼ˆtrain + valï¼‰
        - members_df, transactions_df, logs_df: å…¨é‡æ•°æ®
        - expire_df: åŒ…å« msno å’Œ last_expire_date çš„ DataFrame
        - batch_size: æ¯æ‰¹å¤„ç†çš„ç”¨æˆ·æ•°
    è¿”å›ï¼š
        - åˆå¹¶åçš„ç‰¹å¾ DataFrame
    """
    all_msno = full_df["msno"].unique()
    batches = [all_msno[i:i+batch_size] for i in range(0, len(all_msno), batch_size)]

    features_list = []
    print(f"ğŸ”„ Generating features in {len(batches)} batches (batch size = {batch_size})")

    for i, batch_msno in enumerate(tqdm(batches, desc="ğŸ§© Feature batches")):
        print(f"ğŸ“¦ Processing batch {i+1}/{len(batches)}: {len(batch_msno)} users")

        batch_df = full_df[full_df["msno"].isin(batch_msno)].copy()
        batch_expire = expire_df[expire_df["msno"].isin(batch_msno)].copy()

        # é˜²æ­¢ SettingWithCopyWarning
        if "last_expire_date" in batch_expire.columns:
            batch_expire["last_expire_date"] = pd.to_datetime(batch_expire["last_expire_date"], errors="coerce")

        batch_features = merge_all_features(batch_df, members_df, transactions_df, logs_df, batch_expire)
        features_list.append(batch_features)

    features = pd.concat(features_list, ignore_index=True)
    print(f"âœ… Feature generation complete. Total rows: {len(features)}")
    return features


def get_or_cache_features(
    cache_path,
    compute_fn=None,
    batch_mode=False,
    batch_args=None,
    force_reload=False
):
    """
    é€šç”¨ç‰¹å¾ç¼“å­˜å‡½æ•°ï¼šæ”¯æŒæ ‡å‡†æ¨¡å¼å’Œåˆ†æ‰¹æ¨¡å¼ã€‚
    å‚æ•°ï¼š
        - cache_path: Parquet ç¼“å­˜è·¯å¾„
        - compute_fn: æ ‡å‡†ç‰¹å¾ç”Ÿæˆå‡½æ•°ï¼ˆå¦‚ merge_all_featuresï¼‰
        - batch_mode: æ˜¯å¦å¯ç”¨åˆ†æ‰¹å¤„ç†
        - batch_args: åˆ†æ‰¹å¤„ç†æ‰€éœ€å‚æ•°å­—å…¸ï¼ˆä¼ ç»™ merge_all_features_in_batchesï¼‰
        - force_reload: æ˜¯å¦å¼ºåˆ¶é‡æ–°ç”Ÿæˆ
    è¿”å›ï¼š
        - pd.DataFrame
    """
    if os.path.exists(cache_path) and not force_reload:
        print(f"ğŸ“¦ Loading cached features from {cache_path}")
        return pd.read_parquet(cache_path)

    print("ğŸ”„ Computing features...")
    if batch_mode:
        assert batch_args is not None, "batch_args must be provided when batch_mode=True"
        df = merge_all_features_in_batches(**batch_args)
    else:
        assert compute_fn is not None, "compute_fn must be provided when batch_mode=False"
        df = compute_fn()

    os.makedirs(os.path.dirname(cache_path), exist_ok=True)
    df.to_parquet(cache_path, index=False)
    print(f"âœ… Saved features to {cache_path}")
    return df
